\documentclass{article}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
<<setup, include=FALSE>>=
set.seed(13)
library(XML)
library(RCurl)
library(curl)
library(stringr)
library(methods)
# also a good place to set global chunk options

library(knitr) # need this for opts_chunk command
opts_chunk$set(fig.width = 5, fig.height = 5)

@ 
\begin{document}
\title{STAT243 Problem set3}
\author{Tianyi Zhang}

\maketitle
\section{Problem1}
The article I chose to read was "Best Practices for Scientific Computing" by Greg Wilson. 
I have a question about automating repetitive tasks in scientific computing. Generally, writing functions in R will automate repetitive tasks. However, in some cases, some tasks are similar in parts instead of purely repeating. In this scenario, do we make all efforts to spend more time writing a function as general (have quiet more inputs) as possible so that it would be suitable for all similar tasks? Another choice is to write similar functions to similar tasks which will decrease the reproducibility, but spend little time on single project. Moreover, the latter approach also corresponds to the advice that optimizing codes after it works correctly.
\newline
My second problem is related to the version control software Git. I found that it was a disaster to use Git and Dropbox (or other sync tools) simutaneously, if I had two computers working on the same project. The dropbox will sync first and then I use git pull there would be a conflict message. 
\section{Problem2}
\subsection{2A}
\begin{paragraph}
In Problem 2a, I was using regular expressions and XML tools
to exract first Debates URLs and Years. In particular, I used
a function called toString.XMLNode to transform my data type in
nodes to string so that I can do regular expressions.\\
Besides, I obeserved that all debates happened in Sep or Ocb, which made it easier for me to grep the date.\\
Finally, I wrote a function called selecturl that took year as an input,\\
and returned the URL for the first debate of that year.
\end{paragraph}

<<cache=TRUE>>=
new_html<-htmlParse("http://www.debates.org/index.php?page=debate-transcripts")
##First observe that the text part of the website starts from <p>
listofnodes<-getNodeSet(new_html,"//p//a")
##toString.XMLNode transforms the list element to string so that
## it could be manipulated using regular expressions
stringnode<-unlist(lapply(listofnodes,toString.XMLNode))
selectyear<-stringnode[grep("1996|2000|2004|2008|2012",stringnode)]
first_html<-selectyear[grep("First",selectyear)]
first_html<-str_replace_all(first_html,".*http","http")
first_html<-str_replace_all(first_html,". title.*","")
Dateinfo<-selectyear[grep("First",selectyear)]
Dateinfo<-as.data.frame.Date(str_extract(
  Dateinfo,"(September|October) \\d+, \\d{4}"))
Speechdataframe<-cbind(as.data.frame(first_html),Dateinfo)
Speechdataframe[,2]=str_replace_all(
  Speechdataframe[,2],"(September|October) \\d+, ","")
colnames(Speechdataframe)<-c("first_URL","Year")

###Write a function about how to extract URL of a year given year as an input
select_url<-function(year){
  return(Speechdataframe[Speechdataframe[,2]==year,1])
}

Speechdataframe
@
\subsection{2B and 2C}
In this section, I took the URLink as an input and returned a dataframe for future use.\\
This dataframe contained speakernames in the first column: like "OBAMA" "ROMNEY" "OBAMA",\\
with no neighborhood the same (means no "OBAMA" "OBAMA"). In the second column, it's the raw text with laugther and applause tags. In the third column, I name it spoken text because it does not contain non-spoken texts. \\
Notice that I did eliminate the speaker names at first of some paragraphs, and to combine neighbor chunks by the same person to one chunk, I used a for loop. (I know groupby option in dplyr is a good option, but I am running a ubuntu with R 3.0, which did not support dplyr)
By doing this, I can easily take subset of each candidate by data frame operations.
\newline
<<cache=TRUE>>=
textbody<-function(year){
  speech_data<-htmlParse(select_url(year))
  ## By inspecting the Xpath Code of the element in Chrome.
  ## //p/text() will extract the body of the article
  text_data<-xpathSApply(speech_data,"//p/text()",xmlValue)
  ##Good Look
  # cat(paste(text_data,collapse="\n\n"))
  #This step concatenate all text together, and I extract all speaker names
  ## Then I split the original text by "Speakernames:", and throw out the first elemment of the list
  ## After that I created a data frame with names on the left and text on the right
  text_data<-paste(text_data,collapse=" ")
  text_data<-gsub(pattern="\"","",text_data)
  snames<-as.list(str_replace(unlist(str_extract_all(text_data,"[A-Z]+:")),":",replacement=""))
  text_data<-str_split(text_data,pattern = "[A-Z]+: ")
  text_data<-unlist(text_data)[-1]
  
  finalframe<-data.frame(cbind(unlist(snames),text_data),stringsAsFactors = FALSE)
  index=1
  index_vec<-c(1)
  for(i in 2:nrow(finalframe)){
    if(finalframe[i,1]!=finalframe[i-1,1]){
      index=i
      index_vec<-c(index_vec,index)
    }
    if(finalframe[i,1]==finalframe[i-1,1]){
      finalframe[index,2]=paste(finalframe[index,2],finalframe[i,2],collapse="\n")
    }
  }
  finalframe<-finalframe[index_vec,]
  rownames(finalframe)<-NULL
  ##This line of code eliminates the non-spoken text. Such as Laughter...
  ## I am, However, willing to retain those information. Thus I place the new text in a new column
  ## By saying that, I will compute the # of tags for each candidate in future steps.
  finalframe[,3]<-str_replace_all(finalframe[,2],
                                  "\\(LAUGHTER\\)|\\(APPLAUSE\\)|\\(Applause\\)|\\(Laughter\\)|\\(CROSSTALK\\)","")
  colnames(finalframe)<-c("speakernames","raw text","spoken text")
  return(finalframe)
}

@
\subsection{2D}
In this section, I created a function called splitword to split the text into words and add it as the fourth column in my dataframe, the sentence split is in the fifth column. For simplicity, I will just show the first few words and first few sentences Obama said in 2012 to show that my split is useful. 
\newline
<<cache=TRUE>>=
split_word<-function(finalframe){
  withoutpunc<-str_replace_all(finalframe[,3],pattern="\\.|\\,|\\.\\.\\.|\\?|\\!|\\ --|\\ (?![A-Za-z0-9])","")
  # wordsplit<-lapply(withoutpunc,function(x){return(str_split(x,pattern="\\ "))})
  wordsplit<-str_split(withoutpunc,pattern = "\\ ")
  finalframe[,3]<-str_replace_all(finalframe[,3],"Mr\\.","Mr")
  finalframe[,3]<-str_replace_all(finalframe[,3],"Dr\\.","Dr")
  sentencesplit<-str_split(finalframe[,3],pattern = "\\. |\\! |\\? |\\.\\.\\. ")
  newframe<-cbind(as.list(finalframe[,1]),as.list(finalframe[,2]),
                  as.list(finalframe[,3]),wordsplit,sentencesplit)
  finalframe<-newframe
  colnames(finalframe)<-c("speakernames","raw text","spoken text","wordsplit","sentencesplit")
  finalframe<-data.frame(finalframe,stringsAsFactors = FALSE)
  return(finalframe)
}
example<-textbody(2012)
example<-split_word(example)
##First few words or sentences OBAMA said in 2012 to
## show that I am able to correspond words to candidates
head(unlist(example[example[,1]=="OBAMA",4]))
head(unlist(example[example[,1]=="OBAMA",5]))

@
\subsection{2E and 2F}
In this section, I made a function that would take finalframe from last step, and count the words of each candidate and other basic statistics like number of laughters and applauses. To achieve this I start with an empty data frame with all row names and columnames set, then I insert the result to these dataframe by counting the number of occurence using regular expressions. Notice that it's still complex for me to use lapply here because I use regular expression over different columns of my dataframe.
\newline
<<>>=
##Part E and F, and Also count the number of tags
###Write a function that will return the data required for a speech.
Candidate_stat<-function(finalframe){
  ##Store speaker names to a vector
  speaker_unique<-unlist(unique(finalframe[finalframe[,1]!="SPEAKERS",1]))
  ##Create an empty data frame to store number of words, average length, etc.
  candidate_data<-data.frame(matrix(numeric(0),ncol=17,nrow=3),stringsAsFactors=FALSE)
  colnames(candidate_data)<-c("wordcount","charachtercount","averagelength",
                              "I","we","American","democracy","republic",
                              "Democrat","Republican","freedom",
                              "war","Jesus","God","GodBless","Laughter","Applause")
  rownames(candidate_data)<-speaker_unique
  ##Now all splitting in word is in the third column of the finalframe
  ## for loop looping from 1 to 3, namely moderator and each candidate
  ## The regexvector contains the basic regular expressions for use, some special ones 
  ## will be dealt with seperately.
  regexvector<-c("I[^a-z]","[W|w]e[^a-z]","American?","democracy\\b|democratic\\b",
                 "[R|r]epublic\\b","Democrats?[ic]?","Republicans?",
                 "[F|f]ree[dom]?","[W|w]ars?","Jesus|Christs\\b|Christians?")
  for (i in 1:length(speaker_unique)){
    name=speaker_unique[[i]]
    word_candidate=unlist(finalframe[finalframe[,1]==name,4])
    text_candidate=unlist(finalframe[finalframe[,1]==name,3])
##In order to count Laughters and Applause tags
    raw_candidate=unlist(finalframe[finalframe[,1]==name,2])
##First 3 columns    
    candidate_data$wordcount[i]<-length(word_candidate)
    candidate_data$charachtercount[i]<-sum(nchar(word_candidate))
    candidate_data$averagelength[i]=candidate_data$charachtercount[i]/candidate_data$wordcount[i]

    for (k in 1:length(regexvector)){
      candidate_data[i,k+3]<-sum(str_count(word_candidate,pattern=regexvector[k]))
    }

    #### Since God bless has two words, we need to use main text to count.
    candidate_data$God[i]<-sum(str_count(text_candidate,"[G|g]od (?!bless)"))
    candidate_data$GodBless[i]<-sum(str_count(text_candidate,"[G|g]od bless"))
    ###This is one of part c in the problem.
    candidate_data$Laughter[i]<-sum(str_count(raw_candidate,"\\(LAUGHTER\\)|\\(Laughter\\)"))
    candidate_data$Applause[i]<-sum(str_count(raw_candidate,"\\(APPLAUSE\\)|\\(Applause\\)"))
  }
  return(candidate_data)
}

###Combine all functions together, the stat table is the table of statistics
main<-function(year){
  finalframe<-textbody(year)
  aftersplit<-split_word(finalframe)
  stat_table<-Candidate_stat(aftersplit)
  rownames(stat_table)<-paste(rownames(stat_table),year)
  return(stat_table)
}
result<-lapply(c(2012,2008,2004,2000,1996),main)
result
@
From the table, we have obeserved that 2008 is an unusual case that every part of the script has been counted twice, so the statistics do. Namely, all statistics are even numbers, and it's unlucky that html is not structured. Besides, We can observe that "war" was mentioned significantly more times in 2004, and probably because the happening of the Iraq war. Bush also mentioned freedom a lot in 2004, which also related to the Iraq War. Besides in 2012, Obama got more laughters. 
Another interesting fact is about the average word length, candidates typically had a average word length of 4.5, which is less than the average word length in typical English documents (5.1), which probably because people tend to say easier and shorter words than writing.
\section{Problem 3}
\subsection{3 A and B}
Here I created a function called random walk without using for loops. One thing need to mention is that this function handles gracefully with wrong inputs such as nonintegers, negative numbers, etc.
\newline
<<cache=TRUE>>=
set.seed(11)
randomwalk<-function(nstep=10,start=c(0,0),fullpath=TRUE){
  if (is.numeric(nstep) & nstep%%1==0 & nstep>0){
    randomvector=sample(c("Up","Down","Right","Left"),nstep,replace=TRUE)
    Updown=rep(0,nstep)
    Updown[randomvector=="Up"]=1
    Updown[randomvector=="Down"]=-1
    leftright=rep(0,nstep)
    leftright[randomvector=="Right"]=1
    leftright[randomvector=="Left"]=-1
    xcoordinates<-cumsum(leftright)+start[1]
    ycoordinates<-cumsum(Updown)+start[2]
    finalpos<-c(xcoordinates[nstep],ycoordinates[nstep])
    finalpath<-cbind(xcoordinates,ycoordinates)
    finalpath<-rbind(start,finalpath)
    rownames(finalpath)<-NULL
    if(fullpath==FALSE){
      return(finalpos)
    }
    else{
      return(finalpath)
    }
  }
  else{
    if(is.numeric(nstep) & nstep%%1!=0){
      stop("Your input should be an integer")
    }
    if(is.numeric(nstep) & nstep<=0){
      stop("Your input should be positive")
    }
    else{
      stop("Your input should be a positive integer")
    }
  }
}
randomwalk(10,fullpath=TRUE)
##Illustration for wrong input
a<-randomwalk(20.5)
b<-randomwalk(-10)
@
\subsection{3C}
Then I use a class constructor to create an S3 class called rw, with two attributes in the object, path and final position.
\newline
<<cache=TRUE>>=
walk <- function(nstep=10,start=c(0,0)){
  # constructor for 'rw' class
  path<-randomwalk(nstep,fullpath=TRUE)
  finalpos<-path[nrow(path),]
  obj <- list(finalpos=finalpos,path=path)
  class(obj) <- 'rw' 
  return(obj)
}
walk1<-walk(50)
attributes(walk1)
@
Here I constructed a print and plot method for rw class. In particular, for the plot part, I use the red point (square) to denote the starting point, and use the triangle point to denote the end point. The detail of code explanation is along side with the code. By doing these, I can use plot() and print() directly to rw class objects.
\newline
<<cache=TRUE>>=
print.rw<-function(obj){
  cat("The starting point is:", toString(obj$path[1,]),"\n")
  cat("The end point is: ", toString(obj$path[nrow(obj$path),]))
}
print(walk1)

plot.rw<-function(obj){
## This step presets an empty plot for future usage.
  plot(0,type="n",xlab="xcoordinate",ylab="ycoordinate",main="Random Walk Plot",
       xlim=range(obj$path[,1]),ylim=range(obj$path[,2]))
  lines(obj$path[,1],obj$path[,2])
  points(cbind(obj$path[1,1],obj$path[1,2]),col="red",pch=23)
  points(cbind(obj$path[nrow(obj$path),1],obj$path[nrow(obj$path),2]),col="blue",pch=24)
}

##more steps will bring more pretty plots
walk1<-walk(5000)
plot(walk1)
@
In this section I created a replacement method start and an operator method to find the ith step. I notice that for the start part, we have to minus the orginal starting point coordinates, so that these operations can be done multiple times.
\newline
<<cache=TRUE>>=
`start<-` <- function(object ,...) UseMethod("start<-");

`start<-.rw` <- function(obj, value){
  obj$path[,1]=obj$path[,1]+value[1]-obj$path[1,1]
  obj$path[,2]=obj$path[,2]+value[2]-obj$path[1,2]
  return(obj)
}
start(walk1)<-c(5,7)
##Print first ten rows of object path for illustration
walk1$path[1:10,]

'[.rw'<-function(object,i){
  obj<-object$path
  class(obj)<-"matrix"
  return(obj[i+1,])
}
walk1[3]
@
\end{document}