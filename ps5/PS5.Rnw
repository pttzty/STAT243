\documentclass{article}
\usepackage{geometry}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
<<setup, include=FALSE>>=
library(knitr) # need this for opts_chunk command
library(microbenchmark)
library(pryr)
options(digits=22)
opts_chunk$set(fig.width = 5, fig.height = 5)

@ 
\begin{document}
\title{STAT243 Problem set 5}
\author{Tianyi Zhang}

\maketitle
\section{Problem 1}
\subsection{1A}
At most 16 digits accuracy are actually stored if we want to store 1+1e-12.
<<>>=
1+1e-12
1+(1e-16)*10000
1.000000000001
@
\subsection{1B}
The sum() function gives a slightly different anwser than the result in (a). It has 16 digits of accuracy stored.
<<>>=
vec_R<-c(1,rep(1e-16,10000))
sum(vec_R)
@
\subsection{1C}
Python gives a different answer than R, it has 15 digtis accuracy when using the sum function.
<<engine='python'>>=
import numpy as np
import decimal
vec_python=np.array([1e-16]*(10001))
vec_python[0]=1
decimal.Decimal(np.sum(vec_python))

## Decimal('1.0000000000009985345883478657924570143222808837890625')
@
\subsection{1D}
In R, after I wrote a for loop to do the summation, I found that putting 1 at the \textbf{last} position of the vector will give an accurate result (The maximum digits accuracy that a computer can store, 16 digits accuracy). However, putting 1 in the first position will not give a right answer, it will simply return a 1 for the result of the summation.
<<>>=
function_R<-function(vec_R){
  a=vec_R[1]
  for (i in 2:length(vec_R)){
    a=a+vec_R[i]
  }
  return(a)
}
## Vec_first has one at its first position, and vec_last has one at the last.
vec_first<-c(1,rep(1e-16,10000))
vec_last<-c(rep(1e-16,10000),1)
function_R(vec_first)
function_R(vec_last)
@
In Python, when I wrote the loop with the same functionality. It almost yields the same result as R. When I put 1 in the \textbf{last} position of the array, the for loop gives the most accurate result (16 digits accuracy). When 1 is in the first position, the answer is wrong,(just 1.0 is returned).
\newline
<<engine='python'>>=
import numpy as np
import decimal
def function_python(vec_python):
  a=0
  for i in range(len(vec_python)):
    a=a+vec_python[i]
  return(a)

# This array has one as its first element.
vec_first=np.array([1e-16]*(10001))
vec_first[0]=1
# This array has one as its last element.
vec_last=np.array([1e-16]*(10001))
vec_last[10000]=1

# Result
print decimal.Decimal(function_python(vec_first)) 
print decimal.Decimal(function_python(vec_last)) 

@
\subsection{1E}
It is obvious that the sum() function \textbf{does not simply add numbers from left to right.} The following two will give the accurate results, while adding left to right will give a very different result from the for loop in 1D. 
<<>>=
vec_first<-c(1,rep(1e-16,10000))
vec_last<-c(rep(1e-16,10000),1)
sum(vec_first)
sum(vec_last)
@
\subsection{1F}
My guess is that the sum() function does not care about the order in a vector, since I have tried inserting 1 in mulitple positions in a vector, and all of them will return the same result. 
I have searched a number of documentations stating that simply summing a sequance of n (finite) number of floating numbers has the \textbf{worst} case precision, because the error grows with n. We know the sum() function in R basically called the sum function in C, which involves so-called compensated summation method, which sometimes carry arbitrary re-ordering of the sequence.\\
Reference: \url{http://www.drdobbs.com/floating-point-summation/184403224}
\section{Problem 2}
First I compared the difference in the calculation speed in the vector level, including the addition, multiplication and subsetting. Generally, if we were using double to carry the calculation, it would be faster than do it with integer, which slightly contradicts to what have been talked in class. My guess is that R tends to prevent integer overflow, and thus convert all integers to double before the calculation and then convert them back after the calculation. In R, the largest integer representable is $2*10^9$.
One should notice that subsetting will yield nearly the same amount of time for integers and doubles, which makes sense because the operations of subsetting do not involve the conversion between integers and double. 
<<>>=
options(digits=7)
float_set<-as.double(1:10000)
is.double(float_set)
int_set<-as.integer(1:10000)
is.integer(int_set)
subset_sample<-sample(10000,1000)
object_size(int_set)
object_size(float_set)
## Vector multiplication (elementwise), the final result's type remains the same
## as before arithmatic operations
microbenchmark(a<-int_set*int_set,b<-float_set*float_set)
typeof(a)
typeof(b)
microbenchmark(int_set+int_set,float_set+float_set)

##Vector subsetting, there is no difference in the time of vector subsetting
microbenchmark(int_set[subset_sample],float_set[subset_sample])

@

Now let us assess the Time spent for matrix operations (in linear algebra and elementwise). It takes roughly the same time to do matrix mulitiplication for floating point and integers, and we found that their result type were both double. Floating will be a little faster because the calculation will first transform the integer matrix to double in case of \textbf{integer overflow}, but that time spent is trivial compared to the time spent for matrix multipilication. To illustrate this, floating point calculation is faster than integer calculation in elementwise multiplication, the time difference is more significant because elementwise multiplication itself is lighter. Thus converting integer to double spends a larger proportion of time.
Notice that the subsetting operations for matrices with integers and doubles will have the same speed for the same reason in the vector subsetting, there is no need for conversion.
One interesting try is to compute the inverse of a matrix with integer and floatings, and it almost yield the same time probably because the function solve is smart, and it does not take too long to perform the conversion.
<<cache=TRUE>>=
options(digits=7)
## Create a matrix with 100 rows and 100 columns
int_mat<-matrix(int_set,nrow=100,byrow=TRUE)
float_mat<-matrix(float_set,nrow=100,byrow=TRUE)
typeof(int_mat)
typeof(float_mat)
sample2<-sample(100,50)
## Matrix multiplication
microbenchmark(x<-int_mat%*%int_mat,y<-float_mat%*%float_mat)
typeof(x)
typeof(y)
## Elementwise multiplication
microbenchmark(x<-int_mat*int_mat,y<-float_mat*float_mat)
typeof(x)
typeof(y)

## Matrix subsetting 
microbenchmark(int_mat[sample2,],float_mat[sample2,])

## Taking inverse of a matrix
sample_mat<-sample(20000,100)
random_mat_int<-matrix(as.integer(sample_mat),nrow=10)
random_mat_float<-matrix(as.double(sample_mat),nrow=10)
typeof(random_mat_float)
typeof(random_mat_int)
microbenchmark(solve(random_mat_int),solve(random_mat_float))
@

Note: I have consulted the problem in this problem set with Yueqi Feng, Boying Gong and Jianglong Huang.
\end{document}
