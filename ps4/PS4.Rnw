\documentclass{article}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
<<setup, include=FALSE>>=
# also a good place to set global chunk options
library(knitr) # need this for opts_chunk command
library(microbenchmark)
library(pryr)
opts_chunk$set(fig.width = 5, fig.height = 5)

@ 
\begin{document}
\title{STAT243 Problem set3}
\author{Tianyi Zhang}

\maketitle
\section{Question1}
\subsection{1A}
For the debugging of part A, I add several print statements to record the change of random.seed to see if the position is updated. By using the interactive debugging tool browser function, I use the where function in the pryr pakcage to see that the random.seed is from the <environment: 0x7f98237ade70> but not the global environment. I also find that after runif, the position of randomseed has not get updated, because we did not load from the global environment. Notice that because the browser function requires the interactive interface, so I just wrote the code but disable the running.
\newline
<<>>=
set.seed(0) 
runif(1)
save(.Random.seed, file = 'tmp.Rda')
runif(1)
load('tmp.Rda') 
runif(1)
tmp = function() { 
  load('tmp.Rda')
  print("The position of randomseed after loading the tmp file")
  print(.Random.seed[2])
#   browser()
#   where(".Random.seed")
  print(runif(1))
  print("The position of randomseed after calling runif(1)")
  print(.Random.seed[2])
} 
tmp()

@
\subsection{1B}
To revise the function, I add a statement of env=.Globalenv to aviod loading from the last parent frame. Here when I run runif(1), it will return me the correct value, and random.seed will get update within the function.
\newline
<<>>=
set.seed(0) 
runif(1)
save(.Random.seed, file = 'tmp.Rda')
runif(1)
load('tmp.Rda') 
runif(1)
tmp = function() {
######change here
  load('tmp.Rda',env = .GlobalEnv) 
  print("The position of randomseed after loading the tmp file")
  print(.Random.seed[2])
#   browser()
  print(runif(1))
  print("The position of randomseed after calling runif(1)")
  print(.Random.seed[2])
} 
tmp()
@
\section{Problem 2}
\subsection{2A}
First we evaluate the denominator in this section.We need to take log to 
each component of the denominator to avoid calculation for big factorials. For example, 2000 choose 100 will produce the factorial of 2000 in the numerator, which will be regarded as infinity in R. Also I found that there is a part in the denominator that is the inverse of the other, so we can compute it first and then combine terms. Notice that we need to regard k to be zero and n as special cases.
\newline
<<cache=TRUE>>=
eval_deno<-function(n=10,p=0.3,phi=0.5){
  compute_deno<-function(k){
    part_two<-k*log(k)+(n-k)*log(n-k)-n*log(n)
    main_log=lchoose(n,k)+(1-phi)*part_two+k*phi*log(p)+(n-k)*phi*log(1-p)
    return(exp(main_log))
  }
  ##Case k=0
  first_ele<-n*phi*log(1-p)
  ## Case k=n
  last_ele<-n*phi*log(p)
  ## This vector contains all components from k value 1 to n-1
  main_vec<-sapply(1:(n-1),compute_deno)
  ## This is the denominator
  deno<-sum(main_vec)+exp(first_ele)+exp(last_ele)
  return(deno)
}
@
\subsection{2B}
In this section, we will eliminate any loops or apply functions in our function. Thus we set up a vector from 1 to n minus one, and do the arithematic operations toward this vector. We found that it took significantly less time after vectorizing (about 50 times faster compared to non-vectorize time spent). It took about 400 microseconds to finish the case when n=2000 in a macbook Air, and notice that the result varied significantly between different machines. This same chunk of code for n=2000 only spent 200 microseconds on a newest Macbook Pro machine.
\newline
<<cache=TRUE>>=
vec_deno<-function(n=10,p=0.3,phi=0.5){
  vec<-1:(n-1)
  part_two<-vec*log(vec)+(n-vec)*log(n-vec)-n*log(n)
  main_log=lchoose(n,vec)+(1-phi)*part_two+vec*phi*log(p)+(n-vec)*phi*log(1-p)
  first_ele<-n*phi*log(1-p)
  last_ele<-n*phi*log(p)
  deno<-sum(exp(main_log))+exp(first_ele)+exp(last_ele)
  return(deno)
}
microbenchmark(eval_deno(10),eval_deno(2000),
               vec_deno(10),vec_deno(2000),times=20L)
@
\subsection{2C}
After using Rprof() to detect which line has used significantly more time, I found that lchoose takes the majority use of time. Thus I revised the lchoose to \textbf{lfactorial}, then the time use has been significantly reduced. It took me about 300 milliseconds to run n=2000 in a macbook Air, and the same chunk of code lasts 150 milliseconds in a more powerful Ubuntu machine.
<<>>=
vec_revise<-function(n=10,p=0.3,phi=0.5){
  vec<-1:(n-1)
  part_one<-lfactorial(n)-lfactorial(vec)-lfactorial(n-vec)
  part_two<-vec*log(vec)+(n-vec)*log(n-vec)-n*log(n)
  main_log=part_one+(1-phi)*part_two+vec*phi*log(p)+(n-vec)*phi*log(1-p)
  first_ele<-n*phi*log(1-p)
  last_ele<-n*phi*log(p)
  deno<-sum(exp(main_log))+exp(first_ele)+exp(last_ele)
  return(deno)
}
vec_revise(2000)
#####Improvement here
microbenchmark(vec_revise(2000),vec_deno(2000))
@
\section{problem 3}
\subsection{3A}
The sapply approach is straightforward calculation. I print the head of the object I returned to confirm the final results.
<<>>=
mixedMember<-load("mixedMember.Rda")
result_A_A<-sapply(1:length(IDsA),sumA<-function(x) sum(muA[IDsA[[x]]]*wgtsA[[x]]))
head(result_A_A)
result_A_B<-sapply(1:length(IDsB),sumB<-function(x) sum(muB[IDsB[[x]]]*wgtsB[[x]]))
head(result_A_B)
@
\subsection{3B}
Here I observed that a vector can take a matrix as a subset. Thus I creates an ID matrix with NA as the default element, with 100000 columns and 8 rows. I will use this ID matrix to perform subset of the muA in the computation steps, we will not worry about NA values in the matrix, because we will eventually set na.rm=TRUE in the option in the colsums step to eliminate the effect of NA. muA[IDAmatrix] is a vector with length 800000, where I can perform elementwise multiplication with the weight matrix which has 800000 elemtents, where I also create a weight matrix with 100000 columns and 8 rows.
<<>>=
wgA_mat <- matrix(0, nc = length(wgtsA), nr = max(sapply(wgtsA,length)))
for(j in 1:length(wgtsA)) wgA_mat[,j][1:length(wgtsA[[j]])] <- wgtsA[[j]]

IDAmatrix<-matrix(NA, nc = length(IDsA), nr = max(sapply(IDsA,length)))
for(i in 1:length(IDsA)) IDAmatrix[,i][1:length(IDsA[[i]])] <- IDsA[[i]]
## Adding a zero to the end of the vector
result_CaseA<-colSums(wgA_mat*muA[IDAmatrix],na.rm=TRUE)
head(result_CaseA)
@
\subsection{3C}
Here I create an empty matrix with 10000 rows and 10 columns. It liternally subsititutes the ith row to the ith element with the ID position of weighted list. Then I perform a matrix mulitiplication that returns a vector. There is no need to take the subset of muB because the mulitiplication of zero will return zero. I print the head of the vector to confirm that the result is consisent to the result in part A. 
\newline
<<>>=
##Create an empty matrix to place weight elements, which is the "linear
## transformation" of muB in computation
weightframe <- matrix(0, nr=length(wgtsB), nc=length(muB))
for (i in 1:nrow(weightframe)) {
  weightframe[i, IDsB[[i]]] <- wgtsB[[i]]
}
result_CaseB<-as.vector(weightframe%*%muB)
head(result_CaseB)
@
\subsection{3D}
By looking at the result from R microbenchmarking, we can observe that it takes about 300 milliseconds in average to run sapply for both CaseA and CaseB, because it essentially calls a for loop in C, which is by any means a "stepwise" calculation. However, after vectorizing the calculation for Case A and B. Case A spends about 15 milliseconds, which means about one order magnitude speed up. Case B spends about 2-3 miliseconds, which means about two order magnitude speed up.
\newline
<<>>=
### Comparison for Case A by using sapply and efficient code with matrix computation
microbenchmark(result_A_A<-sapply(1:length(IDsA)
                                  ,sumA<-function(x) sum(muA[IDsA[[x]]]*wgtsA[[x]])),
               result_CaseA<-colSums(wgA_mat*muA[IDAmatrix],na.rm=TRUE),times=10L)

### Comparison for Case B by using sapply and efficient code, respectively.
microbenchmark(result_A_B<-sapply(1:length(IDsB),
                                  sumB<-function(x) sum(muB[IDsB[[x]]]*wgtsB[[x]])),
               result_CaseB<-as.vector(weightframe%*%muB),times=10L)
@
\section{problem 4}
For problem 4, because I am trying to make several print statements inside the raw code of lm function, so the code chunk for each part is fairly long. Thus I concatenate part A,B,C to one part after the result of the output.
<<>>=
library(pryr)
rm(list=ls())
x1<-rnorm(1000000)
x2<-rnorm(1000000)
x3<-rnorm(1000000)
y<-rnorm(1000000)
mem_used()

lmrevised<-function (formula, data, subset, weights, na.action, method = "qr", 
          model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
          contrasts = NULL, offset, ...) 
{
  mem1<-mem_used()
  ret.x <- x
  ret.y <- y
  cl <- match.call()
  mf <- match.call(expand.dots = FALSE)
  m <- match(c("formula", "data", "subset", "weights", "na.action", 
               "offset"), names(mf), 0L)
  mf <- mf[c(1L, m)]
  mf$drop.unused.levels <- TRUE
  mf[[1L]] <- quote(stats::model.frame)
  mf <- eval(mf, parent.frame())
  print("The memroy used after mf is created")
  print(mem_used()-mem1)
  print("The size of mf is")
  print(object_size(mf))
  if (method == "model.frame") 
    return(mf)
  else if (method != "qr") 
    warning(gettextf("method = '%s' is not supported. Using 'qr'", 
                     method), domain = NA)
  mt <- attr(mf, "terms")
  y <- model.response(mf, "numeric")
##
  print(head(.Internal(inspect(y))))
  print("The object size of y is")
  print(object_size(y))
  print("The object size of y names is")
  print(object_size(attributes(y)$names))
  print("The memroy use after y is created")
  print(mem_used()-mem1)
#    attributes(y)$dimnames<-NULL  
##
  w <- as.vector(model.weights(mf))
  if (!is.null(w) && !is.numeric(w)) 
    stop("'weights' must be a numeric vector")
  offset <- as.vector(model.offset(mf))
  if (!is.null(offset)) {
    if (length(offset) != NROW(y)) 
      stop(gettextf("number of offsets is %d, should equal %d (number of observations)", 
                    length(offset), NROW(y)), domain = NA)
  }
  if (is.empty.model(mt)) {
    x <- NULL
    z <- list(coefficients = if (is.matrix(y)) matrix(, 0, 
                                                      3) else numeric(), residuals = y, fitted.values = 0 * 
                y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != 
                                                                                0) else if (is.matrix(y)) nrow(y) else length(y))
    if (!is.null(offset)) {
      z$fitted.values <- offset
      z$residuals <- y - offset
    }
  }
  else {
    print("The memory use after x is created")
    print(mem_used()-mem1)
    x <- model.matrix(mt, mf, contrasts)
    #########
    print(head(.Internal(inspect(x))))
    print("The size of x becomes")
    print(object.size(x))
    print("The size of dimnames of x is")
    print(object_size(attributes(x)$dimnames))
#    attributes(x)$dimnames<-NULL
    #########
    z <- if (is.null(w)){
      print("The memory used at the point lm.fit is called")
      print(mem_used()-mem1)
      lm.fit(x, y, offset = offset, singular.ok = singular.ok, 
             ...)}
    else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, 
                 ...)
  }
  class(z) <- c(if (is.matrix(y)) "mlm", "lm")
  z$na.action <- attr(mf, "na.action")
  z$offset <- offset
  z$contrasts <- attr(x, "contrasts")
  z$xlevels <- .getXlevels(mt, mf)
  z$call <- cl
  z$terms <- mt
  if (model) 
    z$model <- mf
  if (ret.x) 
    z$x <- x
  if (ret.y) 
    z$y <- y
  if (!qr) 
    z$qr <- NULL
  z
}

@
\subsection{4A}
By adding the print memory statement in the raw code of the lm function, we can inspect the memory used at the point that lm.fit is called by adding a print statement right before the lm.fit is called. We can inspect that there is approximately 152 Mb used in the function before the lm.fit called. To eliminate the effect of memory use outside the function, I record the memory usage at the beginning of the function then do the substraction every time I want to record the memory usage. 
\subsection{4B}
Question 1: What objects have used the major memory.\\
Here I need to figure out where is the 152 MB (156MB if running in Rnw compared to just in R) memory usage distributed before the call of the function lm.fit. The way I approach it is to print the size suspicious large objects as well as print the usage of memory after each line. I found that the major use of memory was from the creation of mf, y, and x.\\
mf has the size of 32 MB, and y has the size of 64 MB, while x has the size of 88 MB. However it adds up to 184 MB instead of 152 MB, this is a contradiction that I would like to report here. I even find that the additional 16MB memory use occured in the creation of y, and 48MB less memory use in creation of x.
\newline
Question2: Why are some vectors and matrices take more than 8 bytes per number memory?\\
We know y is a vector with length 1 million, which should take 8MB, however it takes 56MB. The similar condition happens in x, where x has the object size of 88MB, while 400 million numbers should take 8bytes*400 million=32MB.\\
By the command internal inspect, I found that the computer also stores the \textbf{names} of y as an attribute in the RAM, as it's shown in the code output. By printing the object size of name attributes of y, we found that names occupied 56 MB in the memory.\\
Similarly,by internal insepction, I found that x has a large attribute \textbf{dimnames} with size 56MB, which makes it seem that it's not "8 bytes per number".\\
\textbf{From the internal inspect, I also got a rough idea why those object sizes do not add up to the total memory use up to the calling of lm.fit function, I found that the some attributes of x and y (for instance, names) share the same address in the memory, so even though the attribute for each part is large, different objects are sharing memory when they have common attributes.}
\newline
<<>>=
model1<-lmrevised(y~x1+x2+x3)
@
\subsection{4C}
If I am rewriting the lm function, I would make the names of those attributes to be NULL and look at the memory use at the point lm.fit is called. (This time I hide the lm function code because it is too long.) I made my new function name to be lmnew. Now it only takes about 80MB before the call of lm.fit.
\newline
<<echo=FALSE,cache=TRUE>>=
lmnew<-function (formula, data, subset, weights, na.action, method = "qr", 
          model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
          contrasts = NULL, offset, ...) 
{
  mem1<-mem_used()
  ret.x <- x
  ret.y <- y
  cl <- match.call()
  mf <- match.call(expand.dots = FALSE)
  m <- match(c("formula", "data", "subset", "weights", "na.action", 
               "offset"), names(mf), 0L)
  mf <- mf[c(1L, m)]
  mf$drop.unused.levels <- TRUE
  mf[[1L]] <- quote(stats::model.frame)
  mf <- eval(mf, parent.frame())
  if (method == "model.frame") 
    return(mf)
  else if (method != "qr") 
    warning(gettextf("method = '%s' is not supported. Using 'qr'", 
                     method), domain = NA)
  mt <- attr(mf, "terms")
  y <- model.response(mf, "numeric")

  attributes(y)$dimnames<-NULL  
  w <- as.vector(model.weights(mf))
  if (!is.null(w) && !is.numeric(w)) 
    stop("'weights' must be a numeric vector")
  offset <- as.vector(model.offset(mf))
  if (!is.null(offset)) {
    if (length(offset) != NROW(y)) 
      stop(gettextf("number of offsets is %d, should equal %d (number of observations)", 
                    length(offset), NROW(y)), domain = NA)
  }
  if (is.empty.model(mt)) {
    x <- NULL
    z <- list(coefficients = if (is.matrix(y)) matrix(, 0, 
                                                      3) else numeric(), residuals = y, fitted.values = 0 * 
                y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != 
                                                                                0) else if (is.matrix(y)) nrow(y) else length(y))
    if (!is.null(offset)) {
      z$fitted.values <- offset
      z$residuals <- y - offset
    }
  }
  else {
    x <- model.matrix(mt, mf, contrasts)
    attributes(x)$dimnames<-NULL
    #########
    z <- if (is.null(w)){
      print("The memory used at the point lm.fit is called")
      print(mem_used()-mem1)
      lm.fit(x, y, offset = offset, singular.ok = singular.ok, 
             ...)}
    else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, 
                 ...)
  }
  class(z) <- c(if (is.matrix(y)) "mlm", "lm")
  z$na.action <- attr(mf, "na.action")
  z$offset <- offset
  z$contrasts <- attr(x, "contrasts")
  z$xlevels <- .getXlevels(mt, mf)
  z$call <- cl
  z$terms <- mt
  if (model) 
    z$model <- mf
  if (ret.x) 
    z$x <- x
  if (ret.y) 
    z$y <- y
  if (!qr) 
    z$qr <- NULL
  z
}
@
<<>>=
model2<-lmnew(y~x1+x2+x3)
sapply(model2,object.size)
@
\end{document}