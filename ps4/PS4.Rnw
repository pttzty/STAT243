\documentclass{article}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
<<setup, include=FALSE>>=
# also a good place to set global chunk options
library(knitr) # need this for opts_chunk command
library(microbenchmark)
library(pryr)
opts_chunk$set(fig.width = 5, fig.height = 5)

@ 
\begin{document}
\section{Question1}
\subsection{1 A}
<<>>=
set.seed(0) 
runif(1)
save(.Random.seed, file = 'tmp.Rda')
runif(1)
load('tmp.Rda') 
runif(1)
tmp = function() { 
  load('tmp.Rda') 
  runif(1)
} 
tmp()
debug(tmp)
.Random.seed[2]
@
\subsection{1 B}
<<>>=
set.seed(0) 
runif(1)
save(.Random.seed, file = 'tmp.Rda')
runif(1)
load('tmp.Rda') 
runif(1)
tmp = function() { 
  load('tmp.Rda',env = .GlobalEnv) 
  runif(1)
} 
tmp()
.Random.seed[2]
@
\section{Problem 2}
\subsection{2A}
First we evaluate the denominator in this section.We need to take log to 
each component of the denominator to avoid calculation for big factorials. For example, 2000 choose 100 will produce the factorial of 2000 in the numerator, which will be regarded as infinity in R. Also I found that there is a part in the denominator that is the inverse of the other, so we can compute it first and then combine terms. Notice that we need to regard k to be zero and n as special cases.
\newline
<<cache=TRUE>>=
###Problem 2
eval_deno<-function(n=10,p=0.3,phi=0.5){
  compute_deno<-function(k){
    part_two<-k*log(k)+(n-k)*log(n-k)-n*log(n)
    main_log=lchoose(n,k)+(1-phi)*part_two+k*phi*log(p)+(n-k)*phi*log(1-p)
    return(exp(main_log))
  }
  ##Case k=0
  first_ele<-n*phi*log(1-p)
  ## Case k=n
  last_ele<-n*phi*log(p)
  ## This vector contains all components from k value 1 to n-1
  main_vec<-sapply(1:(n-1),compute_deno)
  ## This is the denominator
  deno<-sum(main_vec)+exp(first_ele)+exp(last_ele)
  return(deno)
}
@
\subsection{2B}
In this section, we will eliminate any loops or apply functions in our function. Thus we set up a vector from 1 to n minus one, and do the arithematic operations toward this vector. We found that it took significantly less time after vectorizing (about 50 times faster compared to non-vectorize time spent). It took about 400 microseconds to finish the case when n=2000 in a macbook Air, and notice that the result varied significantly between different machines. This same chunk of code for n=2000 only spent 200 microseconds on a newest Macbook Pro machine.
\newline
<<cache=TRUE>>=
vec_deno<-function(n=10,p=0.3,phi=0.5){
  vec<-1:(n-1)
  part_two<-vec*log(vec)+(n-vec)*log(n-vec)-n*log(n)
  main_log=lchoose(n,vec)+(1-phi)*part_two+vec*phi*log(p)+(n-vec)*phi*log(1-p)
  first_ele<-n*phi*log(1-p)
  last_ele<-n*phi*log(p)
  deno<-sum(exp(main_log))+exp(first_ele)+exp(last_ele)
  return(deno)
}
microbenchmark(eval_deno(10),eval_deno(2000),
               vec_deno(10),vec_deno(2000))
@
\section{problem 3}
\subsection{3A}
<<>>=
mixedMember<-load("mixedMember.Rda")
microbenchmark(result_A_A<-sapply(1:length(IDsA),
                                  sumA<-function(x) sum(muA[IDsA[[x]]]*wgtsA[[x]])))
head(result_A_A)
summary(result_A_A)
microbenchmark(result_A_B<-sapply(1:length(IDsB),
                                  sumB<-function(x) sum(muB[IDsB[[x]]]*wgtsB[[x]])))
head(result_A_B)
summary(result_A_B)
@
\subsection{3C}
<<>>=
##Create an empty matrix to place weight elements, which is the "linear
## transformation" of muB in computation
weightframe <- matrix(0, nr=length(wgtsB), nc=length(muB))
for (i in 1:nrow(weightframe)) {
  weightframe[i, IDsB[[i]]] <- wgtsB[[i]]
}
microbenchmark(result_C<-as.vector(weightframe%*%muB))
head(result_C)
summary(result_C)
@
\section{problem 4}
<<>>=
# rm(list=ls())
# x1<-rnorm(1000000)
# x2<-rnorm(1000000)
# x3<-rnorm(1000000)
# y<-rnorm(1000000)
@
\end{document}